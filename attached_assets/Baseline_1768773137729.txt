3.1 Referenzsystemanalyse: OpenAI Shopping Assistant als Baseline (Workflow + Nudge-Inventar)

OpenAI Shopping Research (häufig als Shopping Assistant bezeichnet) wurde Ende 2025 als neues Feature in ChatGPT eingeführt, um Nutzern die produktbezogene Recherche und Beratung im Chat zu ermöglichen. Laut OpenAI erstellt der Assistent durch gezielte Rückfragen und Web-Recherchen innerhalb weniger Minuten einen personalisierten Kaufberater („Buyer’s Guide“) für die gestellte Anfrage. In dieser Arbeit wird das System als Referenz genutzt, um einen realen, aktuell verfügbaren Shopping-Agent-Workflow zu dokumentieren und daraus eine Baseline-Spezifikation für den eigenen Prototyp „Controlled Shopping Research Assistant“ abzuleiten. Das Referenzsystem dient dabei als Beobachtungs- und Ableitungsgrundlage, ist jedoch keine Experimentalbedingung der späteren Nutzerstudie. Ziel dieses Abschnitts ist (1) die nachvollziehbare Beschreibung des tatsächlichen Workflows (inklusive aller UI-Zustände), (2) die Identifikation der sichtbaren und funktionalen Nudges (d.h. ein Nudge-Inventar des Systems) sowie (3) die Ableitung, welche Elemente im Prototyp 1:1 als „Baseline-UI“ übernommen werden und welche Mechanismen als gezielte Variation (End-Nudge-Treatment) in Frage kommen.

Die Dokumentation basiert auf vier manuellen Walkthroughs mit identischem Startprompt („Ich möchte Kaffee kaufen. Bitte starte eine Shopping-Recherche.“). Die Walkthroughs variieren systematisch in der Nutzerinteraktion: zwei Durchläufe ohne aktive Auswahl in den vorgeschalteten Anforderungs-Kacheln (Requirements), sowie zwei Durchläufe mit aktiver Auswahl (z. B. Budget und Zubereitungsart) und zusätzlichem Feedback in der „Preview & Rate“-Phase (z. B. Interaktion mit More like this / Not interested-Optionen, inklusive Angabe von Ablehnungsgründen). Alle Beobachtungen wurden als fortlaufendes Protokoll festgehalten – einschließlich wörtlicher UI-Texte, System-Statusanzeigen (etwa “Updating Search …”, “Clarifying Product Pricing”), Ergebnis-Headern (z. B. “Shopped for X min · Y products viewed”) sowie auffälliger Formatierungs- und Metadaten-Phänomene in Produktkarten.

3.1.2 Einstieg und Modus-Aktivierung

Die Nutzung des Shopping-Assistenzmodus kann entweder durch eine passende Nutzeranfrage oder manuell über die Modusauswahl im ChatGPT-Interface initiiert werden. Im vorliegenden Fall wurde der Modus via Startprompt explizit aktiviert. Zunächst wählt der Nutzer in der ChatGPT-Plus-Oberfläche den Modus “Shopping Assistant” aus dem Plus-Menü aus (siehe Abbildung 1). Anschließend wird die Eingabe des Startprompts (hier: der Kaufwunsch für Kaffee) vom System erkannt und es erfolgt der automatische Übergang in den speziellen Shopping-Workflow. Unmittelbar nach Absenden des Prompts zeigt die Anwendung oberhalb des Chatverlaufs einen temporären Status an, etwa “Starting shopping research…”, gefolgt von “Gathering requirements…”, was den Wechsel in den Recherchemodus visualisiert. Abbildung 2 zeigt diesen Startzustand des Referenzsystems: Der Nutzerprompt ist bereits gesendet, und das System signalisiert den Beginn der assistierten Shopping-Recherche durch entsprechende Statuszeilen. In diesem Moment sind noch keine Ergebnisse oder Fragen sichtbar – die Interaktion wechselt jedoch von der freien Chat-Eingabe in einen strukturierten Dialogmodus.

[Abbildung 1 hier einfügen]
Abbildung 1: Modusauswahl in ChatGPT (Plus-Menü) mit aktivierter Option „Shopping Assistant“ (Hervorhebung). – Der ChatGPT-Client ermöglicht über das Plus-Menü die Auswahl verschiedener Modes. Für den Einkaufsassistenten wird der Modus Shopping Assistant gewählt, bevor die Anfrage gestellt wird.

[Abbildung 2 hier einfügen]
Abbildung 2: Systemstatus beim Start der Shopping-Recherche. – Unmittelbar nach Eingabe des Startprompts erscheint eine Statusanzeige im Chat („Starting shopping research…“), gefolgt von „Gathering requirements…“. Dies signalisiert, dass das System in den Einkaufssuchmodus gewechselt ist und nun mit der Abfrage von Präferenzen beginnt.

3.1.3 Phase A: Gathering Requirements – Kachelbasierte Präferenzabfrage

In der ersten Phase (“Gathering requirements”) führt das System eine strukturierte Abfrage der Nutzerpräferenzen durch. Nacheinander werden dem Nutzer geschlossene Fragen in Form einer Kachel-UI präsentiert. Typischerweise besteht eine solche Requirement-Eingabe aus einem Satz von vier Auswahlkacheln (angeordnet im 2×2-Raster), ergänzt um einer Kachel “Something else” (für freie Texteingabe oder abweichende Antworten) sowie einer “Skip”-Kachel zum Überspringen. Die Kacheln enthalten Antwortoptionen zur jeweils gestellten Frage (z. B. konkrete Budgetspannen), während Skip ermöglicht, die Frage unbeantwortet zu lassen.

Ein zentrales Charakteristikum ist die Zeit- und Interaktionslogik dieser Abfragen, da hier bereits ein implizites Nudging einsetzt. Bleibt der Nutzer passiv (d.h. weder eine Option anklicken noch manuell Skip wählen), verbleibt die angezeigte Kachelauswahl insgesamt ca. 20 Sekunden auf dem Bildschirm. Nach ungefähr 10 Sekunden Inaktivität erscheint ein kreisförmiger Countdown-Indikator über der Skip-Option. Läuft dieser weitere ~10 Sekunden ab, überspringt das System die Frage automatisch und fährt mit dem nächsten Schritt fort. Mit anderen Worten: Nicht-Handeln führt zwar zum Überspringen der Frage, ist jedoch mit spürbarer Wartezeit verbunden. Dieses Design erzeugt leichte Friktion für passives Verhalten und macht eine aktive Auswahl relativ gesehen „effizienter“ – ein erstes Beispiel für einen Strukturnudge (siehe Abschnitt 3.1.6).

Während der Testläufe wurden über alle Durchläufe hinweg wiederkehrende Requirement-Fragetypen beobachtet. Dazu zählen insbesondere: Budgetrahmen (mit vordefinierten Preisspannen pro Packung oder pro 500 g, je nach Kontext), bevorzugte Bohnenart (etwa Arabica, Robusta, Blend, Spezialität), gewünschter Röstgrad (hell, mittel, dunkel, Espresso/entkoffeiniert) und Zubereitungsart (Filter, Siebträger, French Press, Vollautomat etc.). In einigen Fällen werden auch spezielle Merkmale abgefragt, z. B. “Fairtrade/Bio”, “ganze Bohnen”, “gemahlen” oder “besonders aromatisch”. Die Abfolge kann variieren – im Kaffeebeispiel kam jedoch auffällig häufig frühzeitig die Budgetfrage („Budget für eine Packung?“ bzw. alternativ „Budget für 500 g?“), gefolgt von Fragen zu Sorte, Röstgrad und Zubereitung. Eine Besonderheit in einem Durchlauf war die Budgetabstufung “bis 30 Euro” versus “30 Euro plus”, wodurch der Wert 30 € semantisch zweimal vorkam. Dies stellt zwar keine gezielte Manipulation dar, könnte aber als Ankereffekt wirken (ein Range Framing, bei dem der doppelte Wert 30 € eine Preisankerwirkung entfaltet). Solche Beobachtungen wurden als potentielle Mechanismen notiert, obgleich kein kausaler Effekt daraus abgeleitet werden kann.

Sobald der Nutzer eine Auswahl trifft, wird diese Eingabe unmittelbar vom System verarbeitet. Sichtbar wird dies durch einen kurzen Statuswechsel: Am oberen Rand erscheint z. B. “Updating search: bis 20 Euro” oder “Updating search: Vollautomat”, je nachdem, welche Kachel bestätigt wurde. Das System signalisiert damit, dass der entsprechende Filter oder Präferenzwert übernommen wurde, bevor es – sofern weitere Requirements anstehen – die nächste Frage stellt. Nach Abschluss aller Präferenzabfragen (bzw. nach Überspringen, falls der Nutzer alle Fragen ignoriert oder Skip wählt) wechselt der Workflow in die nächste Phase.

[Abbildung 3 hier einfügen]
Abbildung 3: Beispiel einer Requirement-Kachel mit Skip-Option und Countdown. – Das System fragt hier nach dem gewünschten Röstgrad. Vier Antwortkacheln (hell, mittel, dunkel, entkoffeiniert) stehen zur Auswahl, dazu Something else (für freie Eingabe) und Skip. Der kreisförmige Indikator auf Skip signalisiert, dass bei Untätigkeit in Kürze automatisch übersprungen wird (Timeout-Countdown).

[Abbildung 4 hier einfügen]
Abbildung 4: Status-Update nach Auswahl einer Präferenz. – Direkt nach Bestätigung einer Kachel zeigt das System einen kurzen Status, z. B. “Updating search: Vollautomat”. Dadurch wird dem Nutzer rückgemeldet, welcher Filter übernommen wurde, bevor die nächste Frage oder Phase folgt.

3.1.4 Phase B: Preview and Rate – Feedback-Schleife mit Produktkandidaten

Nach Abschluss der Präferenzabfrage leitet das System – anstatt sofort finale Ergebnisse zu präsentieren – eine zweite Interaktionsphase ein. Es erscheint ein deutlich abgegrenzter Block mit dem Hinweis “Give quick feedback to help ChatGPT pick the best options for you” (etwa: „Gib kurzes Feedback, damit ChatGPT die besten Optionen für dich auswählen kann“). Daneben werden bereits exemplarische Produktbilder angezeigt, meist als kleine Collage oder als erstes Produktbild. Unter dieser Einleitung stehen zwei Auswahlmöglichkeiten zur Verfügung: “Preview and rate” und “Skip all”. Die Preview & Rate-Schaltfläche ist visuell hervorgehoben (schwarzer Primär-Button), während Skip all als einfache Text-Option daneben platziert ist. Diese Gestaltung lenkt die Aufmerksamkeit offensichtlich auf die Feedback-Option.

Besonders hervorzuheben ist die Default-Mechanik an dieser Stelle. In mindestens einem beobachteten Durchlauf führte Nicht-Interaktion des Nutzers nicht zum Überspringen, sondern – nach Ablauf eines kurzen Countdowns – automatisch in die Bewertungssequenz („Preview and rate“). Konkret bedeutet dies: Wenn der Nutzer weder aktiv Preview and rate anklickt noch bewusst Skip all wählt, wird er standardmäßig in die Feedback-Schleife „hineingezogen“. Dieser Default-Zustand (im Gegensatz zum einfachen Abbrechen) stellt einen starken strukturellen Nudge zugunsten aktiven Nutzerfeedbacks dar. Nutzer, die unschlüssig oder inaktiv sind, geben so eher Feedback, anstatt die Phase vollständig zu überspringen.

In der Bewertungssequenz selbst werden dem Nutzer nacheinander einzelne Produktkarten präsentiert. Jede Karte enthält typischerweise ein Produktbild (links), den Produktnamen mit Kurzbeschreibung bzw. Kategorie, den Preis, evtl. ein Händler- oder Markenhinweis und – sofern verfügbar – weitere Metadaten. Zu letzteren zählen insbesondere Sternebewertungen und Review-Zahlen (als Indikator für Kundenzufriedenheit und Popularität) oder spezifische Produktattribute (z. B. Bio-Siegel, besondere Merkmale). Die Darstellungstiefe variiert: Einige Karten zeigen umfangreiche Details (inkl. ⭐ Bewertung, Anzahl Reviews, Ausstattungsmerkmale), andere lediglich Minimalinformationen (z. B. nur der Markenname als Meta-Angabe). Dieses Ungleichgewicht in der Informationsdichte wurde deutlich wahrgenommen. Es liegt nahe, dass Produkte mit reichhaltigen Angaben vom Nutzer als glaubwürdiger oder attraktiver wahrgenommen werden könnten – etwa weil eine ⭐ 4,7-Bewertung mit 4.844 Reviews Vertrauen schafft, während bei Produkten ohne solche Angaben eine Einschätzung schwieriger ist. (Diese Diskrepanz wird in Abschnitt 3.1.6 als möglicher Informationsnudge diskutiert, sollte jedoch als Hypothese und nicht als gesicherter Effekt verstanden werden.)

Der Nutzer hat pro vorgestelltem Produkt drei Hauptaktionen zur Auswahl: “Not interested”, “More like this” oder Skip. Mit Not interested lehnt er den gezeigten Vorschlag ab, More like this signalisiert Gefallen bzw. Interesse (und fordert das System auf, ähnlich gelagerte Produkte zu zeigen), während Skip zum Überspringen des aktuellen Produkts dient. In der Praxis fungiert Skip hier ähnlich wie Inaktivität: Es wurde beobachtet, dass auch ohne Klick nach einigen Sekunden das Produkt automatisch verworfen wird (ein Lade-Indikator rotiert an der Skip-Position) und das nächste Produkt erscheint. Die häufigere Interaktion ist jedoch explizit positiv oder negativ:

Positive Rückmeldung (More like this) veranlasst das System, weitere Produkte vorzuschlagen, die dem gewählten in bestimmten Merkmalen ähneln. So führte z. B. ein “More like this” bei einem preisgünstigen, gemahlenen Kaffee dazu, dass im nächsten Schritt ein weiteres preiswertes Markenprodukt (Jacobs Krönung, ~6 €) mit ähnlich guter Bewertung präsentiert wurde. Mehrfache More like this-Angaben können die Produktsuche schrittweise in eine bestimmte Richtung lenken (hier: Fokus auf günstige, gut bewertete Kaffees). Das System zeigt jeweils kurz an, wonach es nun sucht – etwa “Finding more like Fresh Ground Coffee Pack…” – und lädt dann den nächsten Kandidaten.

Negative Rückmeldung (Not interested) bewirkt, dass das aktuelle Produkt abgelehnt und aus dem Kandidatenpool entfernt wird. Unmittelbar nach dem Klick erscheint jedoch ein klärender Dialog: “Why don’t you like this product?” („Warum gefällt dir dieses Produkt nicht?“). Hier werden dem Nutzer mehrere Kategorien zur Auswahl gestellt, z. B. Price, Style, Brand, Features oder Something else. Diese vorgestanzten Gründe dienen einerseits dazu, die Ablehnung zu klassifizieren, andererseits stellen sie selbst einen Nudge dar: Sie senken die kognitive Hürde, einen Grund zu nennen (der Nutzer muss nicht frei formulieren) und lenken das Feedback in systematisch verwertbare Bahnen. Wählt der Nutzer beispielsweise Price als Ablehnungsgrund, so reagiert der Assistent mit einer Anpassung der Produktsuche. In einem Lauf war deutlich zu sehen, wie nach der Wahl von Price als Grund der Status “Clarifying Product Pricing” oben eingeblendet wurde und im Anschluss vermehrt günstigere Produkte vorgeschlagen wurden. Der Assistent interpretierte das Feedback also dahingehend, dass die bisherigen Vorschläge zu teuer waren, und justierte den Suchraum in Richtung niedrigerer Preise. Diese Clarifying-Schritte können als Meta-Nudges verstanden werden: Das System fragt nach und nutzt die Antwort, um die folgenden Optionen besser auf die Nutzerpräferenzen zuzuschneiden.

Während der Preview & Rate-Phase kann der Nutzer zudem jederzeit eine Detailansicht eines Produktes aufrufen. Klickt man auf den kleinen Pfeil oder Link neben dem Produktnamen, öffnet sich ein Overlay mit ausführlichen Produktdetails: mehrseitige Beschreibungstexte, zusätzliche Bilder (Bildergalerie) sowie ein Button “Visit link”, der zur Original-Produktseite des Anbieters führt. Diese Detailansicht ermöglicht dem Nutzer eine vertiefte Prüfung des Produkts und stellt sicher, dass wichtige Informationen (oder die Kaufmöglichkeit) nicht hinter der Chat-Oberfläche verborgen bleiben. Nach dem Schließen der Detailansicht kehrt man nahtlos in die Feedback-Schleife zurück.

Die Preview & Rate-Schleife endet entweder, wenn der Nutzer manuell abbricht (z. B. durch Skip all zu Beginn oder Abbrechen während der Sequenz), oder wenn das System genügend Feedback gesammelt hat bzw. eine interne Logik greift (beispielsweise nach einer bestimmten Anzahl bewerteter Produkte). In einem intensiven Durchlauf wurden insgesamt 13 Produkte nacheinander bewertet (inklusive mehrfacher More like this-Verzweigungen), bevor der Assistent die Phase beendete. Am Abschluss der Schleife erscheint eine kurze Bestätigungsanzeige: “Thanks for the feedback, your final recommendations will be ready soon.” – begleitet von einem Fortschrittsindikator (Ladebalken) und einer kleinen Bildrotation. Diese Übergangssequenz signalisiert dem Nutzer, dass auf Basis des gegebenen Feedbacks nun die endgültige Empfehlungserstellung erfolgt, und macht die kurze Wartezeit angenehmer (siehe dazu auch Nudge-Analyse in Abschnitt 3.1.6).

[Abbildung 5 hier einfügen]
Abbildung 5: Preview and rate-Aufforderung vs. Skip all. – Nach den Requirements erscheint ein Hinweisblock mit Produktvorschau. Der primäre Button Preview and rate (schwarz hervorgehoben) lädt zur Bewertungsrunde ein, während Skip all als Alternative zum Überspringen angeboten wird. Die Gestaltung (Farbgebung, Position) erhöht die Salienz der Feedback-Option.

[Abbildung 6 hier einfügen]
Abbildung 6: Produktkarte in der Bewertungssequenz. – Beispielhafte Darstellung eines Produkts während Preview & Rate: Links ein Produktbild, rechts Titel, Preis (149 €) und Stichworte. Darunter bieten Not interested und More like this die Feedbackmöglichkeiten. Zusätzliche Metadaten (hier z. B. „4,7 ⭐ (4844 Reviews)“, „Marke: Jacobs“, „gemahlen“) liefern Kontext, der teils zwischen Produkten variiert.

[Abbildung 7 hier einfügen]
Abbildung 7: Dialog zur Ablehnungsbegründung. – Nach Klick auf Not interested fragt das System: „Why don’t you like this product?“. Vordefinierte Gründe (Preis, Stil, Marke, Funktionen oder Something else) stehen zur Wahl. Hier wurde Price gewählt, woraufhin das System mit “Clarifying Product Pricing…” reagierte und künftig billigere Artikel zeigte – ein Beispiel für dynamische Anpassung an Nutzerfeedback.

[Abbildung 8 hier einfügen]
Abbildung 8: Produkt-Detailansicht mit externem Link. – Durch Öffnen der Detailansicht sieht der Nutzer umfassende Informationen zum Produkt (Beschreibungstext, weitere Bilder). Unten befindet sich Visit link, um direkt zur Produktseite (z. B. im Shop oder auf einer Partnerplattform) zu gelangen.

3.1.5 Phase C: Kuratierte Ergebnisdarstellung als Buyer’s Guide

Nach Abschluss (oder Abbruch) der Feedback-Schleife generiert der Shopping Assistant einen kuratierten Kaufratgeber (Buyer’s Guide) als finalen Output. Diese Ergebnisdarstellung ist deutlich umfangreicher und strukturierter als eine einfache Liste von Produkten – sie ähnelt einem redaktionellen Beratungstext, der die gefundenen Optionen zusammenfasst und bewertet.

Gleich zu Beginn der Antwort erscheint eine Ergebnis-Kopfzeile, die kontextuelle Informationen zum Rechercheprozess enthält. Dazu gehören die Gesamtdauer und die Anzahl der betrachteten Produkte, z. B.: “Shopped for 3 min · 13 products viewed”. Unmittelbar daneben – in einem der Durchläufe – wurde eine Tag-Leiste mit den gesetzten Constraints angezeigt. Diese Tags repräsentieren die vom Nutzer geäußerten oder implizit ermittelten Hauptpräferenzen, etwa: “Kaffee”, “Fairtrade/Bio”, “Vollautomat”, “bis 10 Euro”. Eine solche Auflistung dient als Personalisierungsmarker und Justification Cue: Der Nutzer sieht auf einen Blick, dass die Empfehlung genau auf seine Angaben zugeschnitten ist, was die Nachvollziehbarkeit und Akzeptanz der resultierenden Vorschläge erhöht.

Darauf folgend bietet der Buyer’s Guide eine Zusammenfassung (oft eingeleitet mit Überschriften wie “Kurzer Überblick” oder “Zusammenfassung”). Darin wird in ein bis zwei Sätzen das Nutzeranliegen paraphrasiert – im Beispiel: „Du willst Kaffee, ideal für Vollautomaten, möglichst fair/bio, und pro Packung nicht über 10 € ausgeben.“ – um den Kontext der Empfehlung klarzustellen.

Als zentrale Empfehlung wird anschließend häufig ein Top-Pick herausgestellt, z. B. betitelt als “Bestes Gesamtpaket” oder “Beste Wahl gesamt”. Dieser Abschnitt nennt ein konkretes Produkt, das der Assistent als besonders passend identifiziert hat, und liefert eine Begründung dafür. Typischerweise folgen bullet-point-Aufzählungen, die positive Eigenschaften dieses Produkts hervorheben (“Warum diese Wahl?”). Beispiele für solche Punkte sind etwa besondere Qualitätsmerkmale, ein herausragendes Preis-Leistungs-Verhältnis, breite Zustimmung in Nutzerrezensionen oder eine einzigartige Kombination der gesuchten Kriterien. Häufig werden diese Stichpunkte durch Fettdruck strukturiert (z. B. “Echte helle Charakteristik, aber nicht nur für Filter:” gefolgt von der Erklärung), um wichtige Vorteile klar erkenntlich zu machen. Anschließend werden unter einer Überschrift wie “Trade-offs” auch potenzielle Nachteile oder Kompromisse des Top-Picks benannt. So erfährt der Leser, in welchen Aspekten das Produkt eventuell Schwächen hat oder für wen es nicht ideal ist. Auch hier sind die Informationen knapp und punktuell formuliert (z. B. “weniger kräftig als dunklere Röstungen; bei sehr hartem Wasser etwas empfindlicher”), um transparent zu machen, welche Abwägungen bestehen.

Neben dem Hauptprodukt präsentiert der Assistent im Anschluss weitere empfohlene Optionen, oft unter einem Titel wie “Weitere sehr gute Optionen” oder “Top Picks”. Diese Alternativen (zweitrangige Empfehlungen) sind durchnummeriert (z. B. 2), 3), etc.) und folgen einem ähnlichen Beschreibungsmuster. Jeder dieser Einträge enthält einen kurzen Titel, der die Produktkategorie und den Nutzen umreißt, sowie wiederum Bullet-Points zu Besonderheiten, Eignung (“für wen besonders gut”) und ggf. Trade-offs. Auf diese Weise entsteht ein mehrstufiger Empfehlungscontent: Ein primärer Vorschlag plus Ergänzungen, sodass unterschiedliche Nutzerpräferenzen abgedeckt sind (falls der Top-Pick doch nicht in allen Punkten überzeugt, bieten die Alternativen andere Schwerpunkte). Zur Veranschaulichung kann ein tabellarischer Vergleich Teil der Ausgabe sein: Im Kaffee-Durchlauf etwa generierte der Assistent eine Tabelle, welche die empfohlenen Röstungen hinsichtlich Aroma-Profil, Preis pro Einheit, Verfügbarkeit und bestmöglichem Einsatzfeld nebeneinanderstellte. Dies erleichtert dem Nutzer, auf einen Blick zentrale Unterschiede zu erkennen (z. B. Geschmacksnoten oder Lieferzeit). Es ist anzumerken, dass diese Tabelle Teil des Antwort-Texts von ChatGPT war – also keine interaktive UI-Tabelle, sondern ein formatiertes Ergebnis innerhalb der Chat-Antwort.

Am Ende der Ausarbeitung findet sich ein interaktives Element “View products”, das dem Nutzer ermöglicht, alle während der Recherche betrachteten Produkte in einer Liste einzusehen. Klickt man auf View products, öffnet sich eine Produktlisten-Übersicht als Overlay. Darin sind sämtliche in Phase B gezeigten Artikel aufgeführt – inklusive derer, die der Nutzer übersprungen oder abgelehnt hat. Bereits gegebenes Feedback wird hier explizit markiert: Produkte, die positiv bewertet wurden (More like this), tragen ein Badge “liked”, abgelehnte ein Badge “not interested”. Zusätzlich werden am oberen Rand Summen angezeigt, z. B. “8 liked, 2 not interested” von insgesamt 13 angesehenen Produkten. Diese Übersicht bietet dem Nutzer Transparenz darüber, welche Artikel in die Entscheidung eingeflossen sind, und ermöglicht es, ggf. doch noch einzelne davon direkt anzusteuern (jede Zeile ist mit einem Link zum Produktdetail versehen).

Schließlich sind in der Ergebnisdarstellung auch die Quellenangaben der recherchierten Inhalte verfügbar. Ähnlich wie im normalen ChatGPT-Browsing-Modus lassen sich bei jedem gelisteten Produkt die zugehörigen Quellenlinks einsehen (in der UI meist über ein “^”-Symbol oder einen Button neben Like/Kopieren/Teilen). Die Quellen beziehen sich auf die Websites, aus denen der Assistent Informationen gezogen hat (z. B. Shop-Seiten, Testberichte, Foreneinträge). Dass diese Links offen zugänglich sind, wirkt als Vertrauens- und Kontrollmechanismus: Der Nutzer kann die Angaben im Buyer’s Guide bei Bedarf direkt validieren und nachvollziehen, was die qualitativen “quality sources” der KI waren. In unserem Kaffee-Beispiel verwies der Assistent etwa auf spezifische Röster-Websites (z. B. Coffee Circle) oder Diskussionsforen (Kaffee-Netz), um Aussagen zu untermauern. Damit unterstreicht das System seine Autorität nicht nur durch Social Proof (Sternebewertungen), sondern auch durch Transparenz der Herkunftsinformationen.

[Abbildung 9 hier einfügen]
Abbildung 9: Ergebnis-Kopfzeile mit Dauer, Produktanzahl und Tag-Leiste. – Ausschnitt aus dem oberen Teil der finalen Antwort. Zu sehen sind die Angaben “Shopped for … minutes, … products viewed” sowie die Tags der gesetzten Filter (Kaffee, Fairtrade/Bio, Vollautomat, bis 10 Euro), die die Personalisierung und Matching-Kriterien der Empfehlung hervorheben.

[Abbildung 10 hier einfügen]
Abbildung 10: Struktur des Buyer’s Guide-Outputs. – Mittelteil der ChatGPT-Ausgabe mit typischer Abschnittslogik: Ein hervorgehobener Top-Pick (Bestes Gesamtpaket) mit Begründungsstichpunkten, gefolgt von Trade-offs und weiteren nummerierten Empfehlungen (Top-Picks 2 und 3). Diese Gliederung lehnt sich an menschliche Kaufberatungs-Artikel an.

[Abbildung 11 hier einfügen]
Abbildung 11: View products-Listenansicht mit Feedback-Markierungen. – Über das View products-Overlay werden alle betrachteten Produkte angezeigt. Hier sind beispielhaft verschiedene Artikel gelistet, einige mit Badge liked (positives Feedback gegeben), andere mit not interested (abgelehnt). Oben rechts sind die aggregierten Feedback-Zähler zu sehen.

[Abbildung 12 hier einfügen]
Abbildung 12: Quellenanzeige in der finalen Antwort. – Die Produktkarte eines empfohlenen Kaffees mit eingeblendeter Quellenliste. Der Nutzer kann hier die hinterlegten Links (z. B. zur Shop-Seite oder einem Forumseintrag) einsehen und anklicken, um Details und Ursprung der Angaben zu verifizieren.

3.1.6 Nudge-Inventar: Zuordnung zu Decision Information, Decision Structure, Decision Assistance und Social Decision Appeal

Die beobachteten Nudging-Mechanismen des Referenzsystems lassen sich anhand der in Kapitel 2 eingeführten Taxonomie einordnen. Diese orientiert sich an Jesse & Jannach (2021) und baut auf der Klassifikation von Münscher et al. (2016) auf. Auf Top-Level werden vier Kategorien unterschieden: Decision Structure, Decision Information, Decision Assistance sowie Social Decision Appeal. Im Folgenden werden die identifizierten UI-Elemente und Interaktionsdesigns des Shopping Assistant diesen Kategorien zugeordnet:

A) Decision Structure (Strukturnudges)

Default-Pfad über Timeout bei Requirements: Bei den Präferenz-Kacheln führt Passivität des Nutzers zum automatischen Überspringen der Frage nach Countdown. Wichtig: Die automatische Auswahl (Skip) erfolgt erst nach ca. 20 Sekunden (Countdown-Indikator nach ~10 s, Ausführung ~10 s später). Dieser zeitverzögerte Default strukturiert den Prozess und setzt implizit einen „Takt“, der aktives Verhalten begünstigt (siehe Phase A).

Default-Pfad über Timeout bei Preview & Rate: In Phase B wurde beobachtet, dass Nicht-Interaktion nicht etwa Preview & Rate überspringt, sondern im Gegenteil als Default in die Bewertungssequenz hineinführt. Das heißt, wer nichts tut, gibt dennoch Feedback. Dies ist ein besonders starker struktureller Default-Nudge zugunsten aktiver Beteiligung (der Nutzer muss aktiv ablehnen, um keine Bewertungen abzugeben).

Reduzierte Auswahlmenge im Ergebnis: Anstatt den Nutzer mit Dutzenden Optionen zu überfordern, bündelt der Buyer’s Guide die Empfehlung auf 5–6 Produkte (meist ein Hauptvorschlag plus ~4 weitere Top-Picks). Diese Choice Reduction ist intentional: Sie vereinfacht die Entscheidung, indem nur eine kuratierte Teilmenge aller gefundenen Produkte präsentiert wird. Unwichtige Optionen filtert das System vorab aus.

B) Decision Information (Informationsnudges)

Framing durch kuratierte Labels: Die Ausgabe nutzt teils gerahmte Überschriften, die die Empfehlung als qualitätsgesichert darstellen – z. B. “kurze, verlässliche Auswahl”, “Bestes Gesamtpaket”, “Warum diese Wahl?”. Dadurch wird dem Nutzer kommuniziert, dass die gezeigten Optionen bereits eine Vorselektion darstellen und vertrauenswürdig sind. Das framing lässt die Entscheidung einfacher erscheinen, da sie als vorgeprüft wahrgenommen wird.

Salienz durch visuelle Hervorhebung: Die Gestaltung des Interfaces betont wichtige Handlungsoptionen. So ist etwa der Button “Preview and rate” in Phase B deutlich hervorgehoben (schwarzer Hintergrund, zentrale Platzierung), was Aufmerksamkeit darauf lenkt. Dieses Salienz-Nudging erhöht die Wahrscheinlichkeit, dass der Nutzer diese Option wahrnimmt und nutzt.

Salienz durch Bild-Microinteractions: In der finalen Ergebnisansicht wurde eine kleine UI-Interaktion beobachtet: Beim Hover über das Haupt-Produktbild tritt ein leichter Zoom-Effekt ein (CSS-Klasse hover:scale-105 wurde im Frontend-Code gefunden). Diese subtile Animation zieht den Blick des Nutzers auf das Produktfoto und kann die Aufmerksamkeit für den Top-Pick steigern. Visuelle Highlights dieser Art zählen zu Informationsnudges, da sie bestimmte Informationselemente (hier das Bild des empfohlenen Produkts) prominenter machen.

Anchoring durch Range-Design bei Budget: Wie erwähnt, tauchte in einer Budgetfrage der Wert 30 Euro zweifach auf (einmal als obere Grenze einer Kategorie, einmal als eigenständige “30+”-Option). Dieses ungewöhnliche Range-Design kann ungewollt einen Anker setzen – Nutzer könnten €30 als Richtwert für eine „normale“ Ausgabe wahrnehmen. Obwohl dies vermutlich ein UI-Artefakt ist, zeigt es ein grundsätzliches Anchoring-Potenzial im Design von Kategorien.

Informationsasymmetrie zwischen Produktkarten: Manche Produktvorschläge enthielten deutlich mehr objektive Informationen (Bewertungen, technische Details) als andere. Solche Unterschiede können als Nudge wirken, indem Optionen mit höherer Informationsdichte vom Nutzer als verlässlicher oder attraktiver empfunden werden. In der Beobachtung gab es z. B. die subjektive Tendenz, eher zu teureren Kaffees zu greifen, wenn diese mit umfangreichen Daten präsentiert wurden, während günstigere mit minimalen Infos versehen waren – hier könnte ein Uncertainty Reduction-Effekt greifen, der jedoch empirisch weiter zu untersuchen wäre.

C) Decision Assistance (Hilfsnudges)

Geführte Entscheidungssequenz (Constraint Elicitation): Der Shopping Assistant reduziert die Komplexität der Kaufentscheidung durch eine schrittweise Präferenzabfrage. Anstatt den Nutzer zu Beginn alle Kriterien selbst formulieren zu lassen, stellt er kurze, sequenzielle Fragen (Budget, Art, etc.). Dieses Constraint Elicitation-Verfahren führt den Nutzer zielgerichtet durch den Entscheidungsraum und entspricht einem klassischen Hilfsnudge: Die Entscheidung wird durch Strukturierung in kleine Schritte erleichtert. (Vergleiche Phase A: kachelbasierter Dialog statt Freitext-Eingabe).

Progress-Feedback beim Abschluss der Feedbackschleife: Die Einblendung “Thanks for the feedback, your final recommendations will be ready soon” mit Ladebalken und Bilderrotation am Ende der Phase B wirkt als Progress Indicator und Abschluss-Signal. Solche Elemente erhöhen die Wahrnehmung eines fortschreitenden Prozesses und können die Wartezeit überbrücken, indem sie dem Nutzer ein Gefühl von Abschluss und baldiger Belohnung (Endergebnis kommt gleich) vermitteln. Dies entspricht bewährten UX-Patterns, die Nutzer bei Laune halten, und kann als Nudge interpretiert werden, da Geduld gefördert wird, statt die Sitzung abzubrechen.

Begründungsstruktur im Buyer’s Guide: Die klare Gliederung der Ergebnispräsentation – insbesondere Abschnitte wie “Warum diese Wahl”, “Trade-offs” oder Hinweise für bestimmte Nutzer – bietet Entscheidungsassistenz. Durch erklärende Texte und das Abwägen von Vor- und Nachteilen wird die wahrgenommene Unsicherheit reduziert. Der Nutzer erhält quasi eine Beratung, die ihn darin unterstützt, selbst eine fundierte Entscheidung zu treffen. Dieser Mechanismus erhöht das Vertrauen in die Empfehlung und hilft, die Entscheidung schneller zu fällen, da wichtige Pros/Cons bereits aufbereitet sind.

Vordefinierte Ablehnungsgründe als Feedbackhilfe: Die “Why don’t you like this product?”-Nachfrage mit vorgegebenen Antwortkategorien (Price/Style/Brand/Features) senkt die Antwortkosten für den Nutzer erheblich. Statt selbst nachdenken zu müssen, kann er mit einem Klick einen Grund angeben. Dies fördert zum einen, dass überhaupt Feedback gegeben wird (weniger Tipp-Aufwand), zum anderen kanalisiert es das Feedback in systeminterpretable Kategorien. Der Assistent kann dadurch gezielt reagieren (z. B. Preise anpassen) – ein gelungenes Beispiel für einen kombinierten Hilfs- und Strukturnudge im Feedbackprozess.

D) Social Decision Appeal (soziale bzw. autoritative Signale)

Social Proof durch Bewertungen: Wo immer verfügbar, blendet das System Kundenbewertungen in Form von ⭐ Ratings und Review-Zahlen ein (z. B. 4,7 Sterne, 4.844 Reviews). Dies stellt einen sozialen Bewährtheitseffekt (Social Proof) dar: Nutzer tendieren eher zu Produkten, die viele andere positiv bewertet haben. Auch Markennamen bekannter Hersteller können hierunter fallen (Vertrauen in bekannte Brands). Diese sozialen Signale beeinflussen die Entscheidung, indem sie eine Art Schwarm-Expertise oder Autorität der Masse suggerieren.

Quellenzugriff als Autoritäts- und Transparenzsignal: Die Möglichkeit, Quellen der Produktinformationen einzusehen (siehe Abbildung 12), wirkt als Vertrauensnudge. Sie suggeriert Transparenz und Überprüfbarkeit – Werte, die Autorität vermitteln. Nutzer können selbst kontrollieren, ob die Empfehlungen auf seriösen Informationen beruhen. In Kombination mit Formulierungen im Text, die Expertise andeuten (z. B. „renommierte Rösterei“ o. ä.), entsteht ein Authority Cue: Die KI erscheint als kompetenter, vertrauenswürdiger Berater, der seine Aussagen belegen kann. Dies kann die Compliance des Nutzers mit den Empfehlungen erhöhen.

3.1.7 Ableitung: Zentrale Baseline-Elemente für den Prototyp

Aus der obigen Analyse lassen sich konkrete Designempfehlungen für den eigenen Baseline-Prototyp ableiten. Ziel ist es, die Schlüsselelemente des OpenAI-Workflows so zu reproduzieren, dass eine vergleichbare User Experience entsteht und die späteren Variationen klar darauf aufbauen. Insbesondere sollten folgende Komponenten 1:1 nachgebildet werden:

Phasenstruktur & Statusanzeigen: Der Chat-Flow sollte klar erkennbare Phasen haben (Start, Requirements-Abfrage, Searching/Finding/Clarifying-Schritte, Abschluss). Kurze Statusmeldungen im Chat (wie “Gathering requirements…”, “Finding options…”) signalisieren dem Nutzer den Prozessfortschritt und erhöhen die Transparenz.

Kachelbasierte Requirements-Abfrage: Die Präferenzabfragen zu Beginn sollten in Form von Kacheln mit typischer 2×2-Auswahl + Something else + Skip erfolgen. Die Timeout-Mechanik mit Countdown ist mitzuberücksichtigen, um denselben Default-Effekt (Überspringen nach Wartezeit) zu erzielen.

“Preview and rate”-Gate: Nach den Requirements ist eine Zwischenstufe einzubauen, bei der der Nutzer optional Produkte bewerten kann. Diese sollte analog umgesetzt werden: mit einem hervorgehobenen Primär-Button für die Feedback-Schleife und einem Skip-Knopf. Wichtig ist hier auch der Default: Im Baseline-Prototyp kann man ebenfalls vorsehen, dass ohne Nutzeraktion automatisch in die Bewertungsrunde gewechselt wird (um das Originalverhalten nachzustellen).

Produkt-Bewertungsloop mit Feedbackdialog: Die Schleife zur Bewertung einzelner Produkte ist ein zentrales Element. Pro Produkt sind Buttons “Not interested” und “More like this” (ggf. plus Skip) vorzusehen. Der Ablehnungsdialog mit vordefinierten Gründen (Preis, etc.) sollte integriert werden, da er ein klares Merkmal des Referenzsystems ist. Dieses sequentielle Refinement der Suche durch Nutzerfeedback gilt es möglichst originalgetreu nachzubilden.

Finaler Buyer’s Guide mit strukturierter Ausgabe: Die abschließende Empfehlung sollte dem OpenAI-Vorbild folgen – also übersichtlich gegliedert sein (Überblick, Top-Empfehlung mit Begründung, Trade-offs, weitere Optionen) und dem Nutzer einen echten Mehrwert bieten. Dazu gehört auch eine View-Products-Liste mit allen begutachteten Artikeln inklusive Feedback-Markierungen, um die Entscheidungsfindung transparent zu machen. Ebenso sollten Quellenangaben nicht fehlen, um Vertrauen zu schaffen. Die textliche Ausgestaltung (Überschriften wie “Bestes Gesamtpaket”, Bullet-Points etc.) kann als Vorlage dienen, damit der Prototyp denselben Duktus einer kuratierten Beratung vermittelt.

Durch die Umsetzung dieser Kernpunkte wird der entwickelte Prototyp funktional und interaktiv eng an der OpenAI-Baseline orientiert sein. So ist gewährleistet, dass etwaige Unterschiede im Nutzerverhalten in späteren Experimenten auf die gezielten Nudge-Variationen zurückgeführt werden können – und nicht auf fundamentale Abweichungen im Interface oder Ablauf.